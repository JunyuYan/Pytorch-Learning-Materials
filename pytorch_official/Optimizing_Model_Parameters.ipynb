{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNyJnYauPPnGcqcYLMB2aXn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JunyuYan/Pytorch-Learning-Materials/blob/main/pytorch_official/Optimizing_Model_Parameters.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a model and data itâ€™s time to train, validate and test our model by optimizing its parameters on our data. Training a model is an iterative process; in each iteration the model makes a guess about the output, calculates the error in its guess (loss), collects the derivatives of the error with respect to its parameters, and optimizes these parameters using gradient descent."
      ],
      "metadata": {
        "id": "g045ZQIoG2le"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters\n",
        "\n",
        "Hyperparameters are adjustable parameters to control the model optimization process. Different hyperparameter values can impact model training and convergence rate.\n",
        "\n",
        "Genrally, we need to define the following hyperparameters:\n",
        "\n",
        "\n",
        "*   Number of Epochs: the number times to iterate over datasets\n",
        "*   Batch size: the number of data samples propogated through the network before the parameters are updated\n",
        "*   Learning Rate: how much to update model parameters at each batch/epoch. Smaller values yield slow learning speed, while large values may result in unpredictable behavior during training\n",
        "\n",
        "\n",
        "Each iteration of an optimization loop is called an epoch.\n",
        "Each epoch consists of two parts:\n",
        "\n",
        "*   The train loop: iterate over the training dataset to make the model converge to optimal parameters\n",
        "*   The validate/test loop: iterate over the test dataset to check if the model's performance is improving\n"
      ],
      "metadata": {
        "id": "o9yD7WdGTKOM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss Function\n",
        "Loss function measures the degree of dissimilarity of obtained result to the target value, and it is the loss function that we want to minimize during training. To calculate the loss we make a prediction using the inputs of our given data sample and compare it against the true data label value.\n",
        "\n",
        "Detailed description of different loss functions in pytorch is listed [here](https://pytorch.org/docs/stable/nn.html#loss-functions)."
      ],
      "metadata": {
        "id": "3wijwwesW9fg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizer\n",
        "Optimization is the process of adjusting model parameters to reduce model error in each training step. Optimization algorithms define how this process is performed.\n",
        "\n",
        "Pytorch supports a number of optimizers, as listed [here](https://pytorch.org/docs/stable/optim.html).\n",
        "\n",
        "Inside the training loop, optimization happens in three steps:\n",
        "\n",
        "* Call ``optimizer.zero_grad()`` to reset the gradients of model parameters. Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration.\n",
        "* Backpropagate the prediction loss with a call to ``loss.backward()``. PyTorch deposits the gradients of the loss w.r.t. each parameter.\n",
        "* Once we have our gradients, we call ``optimizer.step()`` to adjust the parameters by the gradients collected in the backward pass.\n",
        "\n",
        "In the optimizer, we should initialize it with the ``model.parameters`` and ``learning_rate``."
      ],
      "metadata": {
        "id": "b0motl8iYjlp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Full Implementation\n",
        "\n",
        "Now we can build a complete neural network."
      ],
      "metadata": {
        "id": "75O9N84P6uxq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjI9nsbxF6j9",
        "outputId": "990ba0b6-563c-47aa-c71b-ea011af8daa8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model structure: NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "Epoch 1:-------------------------------\n",
            "loss: 2.309927 | current:    64/60000\n",
            "loss: 2.293953 | current:  6464/60000\n",
            "loss: 2.287560 | current: 12864/60000\n",
            "loss: 2.278072 | current: 19264/60000\n",
            "loss: 2.259205 | current: 25664/60000\n",
            "loss: 2.232706 | current: 32064/60000\n",
            "loss: 2.238160 | current: 38464/60000\n",
            "loss: 2.212430 | current: 44864/60000\n",
            "loss: 2.208212 | current: 51264/60000\n",
            "loss: 2.174523 | current: 57664/60000\n",
            "Accuracy: 45.8, Average loss: 2.175831\n",
            "\n",
            "Epoch 2:-------------------------------\n",
            "loss: 2.187833 | current:    64/60000\n",
            "loss: 2.178426 | current:  6464/60000\n",
            "loss: 2.139818 | current: 12864/60000\n",
            "loss: 2.146816 | current: 19264/60000\n",
            "loss: 2.104477 | current: 25664/60000\n",
            "loss: 2.047248 | current: 32064/60000\n",
            "loss: 2.071678 | current: 38464/60000\n",
            "loss: 2.010579 | current: 44864/60000\n",
            "loss: 2.006447 | current: 51264/60000\n",
            "loss: 1.927778 | current: 57664/60000\n",
            "Accuracy: 57.1, Average loss: 1.941119\n",
            "\n",
            "Epoch 3:-------------------------------\n",
            "loss: 1.976673 | current:    64/60000\n",
            "loss: 1.946106 | current:  6464/60000\n",
            "loss: 1.858418 | current: 12864/60000\n",
            "loss: 1.876298 | current: 19264/60000\n",
            "loss: 1.782128 | current: 25664/60000\n",
            "loss: 1.726780 | current: 32064/60000\n",
            "loss: 1.740474 | current: 38464/60000\n",
            "loss: 1.655755 | current: 44864/60000\n",
            "loss: 1.664236 | current: 51264/60000\n",
            "loss: 1.544190 | current: 57664/60000\n",
            "Accuracy: 60.1, Average loss: 1.575738\n",
            "\n",
            "Epoch 4:-------------------------------\n",
            "loss: 1.649515 | current:    64/60000\n",
            "loss: 1.601642 | current:  6464/60000\n",
            "loss: 1.475471 | current: 12864/60000\n",
            "loss: 1.524358 | current: 19264/60000\n",
            "loss: 1.411607 | current: 25664/60000\n",
            "loss: 1.399465 | current: 32064/60000\n",
            "loss: 1.408822 | current: 38464/60000\n",
            "loss: 1.342721 | current: 44864/60000\n",
            "loss: 1.364052 | current: 51264/60000\n",
            "loss: 1.251880 | current: 57664/60000\n",
            "Accuracy: 63.7, Average loss: 1.284698\n",
            "\n",
            "Epoch 5:-------------------------------\n",
            "loss: 1.372285 | current:    64/60000\n",
            "loss: 1.339336 | current:  6464/60000\n",
            "loss: 1.194577 | current: 12864/60000\n",
            "loss: 1.283531 | current: 19264/60000\n",
            "loss: 1.158265 | current: 25664/60000\n",
            "loss: 1.183859 | current: 32064/60000\n",
            "loss: 1.200900 | current: 38464/60000\n",
            "loss: 1.148119 | current: 44864/60000\n",
            "loss: 1.174980 | current: 51264/60000\n",
            "loss: 1.081910 | current: 57664/60000\n",
            "Accuracy: 65.5, Average loss: 1.103150\n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "# Download datasets\n",
        "train_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(train_data, batch_size=64)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64)\n",
        "\n",
        "# Define neural network structure\n",
        "class NeuralNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.linear_relu_stack = nn.Sequential(\n",
        "        nn.Linear(28*28, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 10)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.flatten(x)\n",
        "    logits = self.linear_relu_stack(x)\n",
        "    return logits\n",
        "\n",
        "model = NeuralNetwork()\n",
        "print(f\"Model structure: {model}\")\n",
        "\n",
        "# Deine train and test loop\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  # Set the model to training mode\n",
        "  model.train()\n",
        "\n",
        "  for batch, (x, y) in enumerate(dataloader):\n",
        "    pred = model(x)\n",
        "    loss = loss_fn(pred, y)\n",
        "\n",
        "    # backpropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      loss, current = loss.item(), (batch+1)*len(x)\n",
        "      print(f\"loss: {loss:>7f} | current: {current:>5d}/{size:>5d}\")\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "  # Set the model to evaluatio mode\n",
        "  model.eval()\n",
        "\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batch = len(dataloader)\n",
        "  total_loss, correct = 0, 0\n",
        "\n",
        "  # Disable the gradient tracking to avoid unnecessary gradient computations\n",
        "  with torch.no_grad():\n",
        "    for x, y in dataloader:\n",
        "      pred = model(x)\n",
        "      total_loss += loss_fn(pred, y).item()\n",
        "      correct += (pred.argmax(1)==y).type(torch.float).sum().item()\n",
        "\n",
        "  test_loss = total_loss/num_batch\n",
        "  correct = correct/size\n",
        "  print(f\"Accuracy: {100*correct:>0.1f}, Average loss: {test_loss:>5f}\\n\")\n",
        "\n",
        "# Main function for calling train and test process\n",
        "epoch = 5\n",
        "learning_rate = 1e-3\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), learning_rate)\n",
        "\n",
        "for i in range(epoch):\n",
        "  print(f\"Epoch {i+1}:-------------------------------\")\n",
        "  train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "  test_loop(test_dataloader, model, loss_fn)\n",
        "\n",
        "print(\"Done!\")\n",
        "\n"
      ]
    }
  ]
}