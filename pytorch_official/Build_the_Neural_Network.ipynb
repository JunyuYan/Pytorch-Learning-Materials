{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7baP4rnDM69WVlxzrSaId",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JunyuYan/Pytorch-Learning-Materials/blob/main/pytorch_official/Build_the_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural networks comprise of layers/modules that perform operations on data.\n",
        "\n",
        "Two important concepts:\n",
        "\n",
        "\n",
        "> torch.nn: provides all building blocks you need to build the neural network\n",
        "\n",
        "\n",
        "> nn.Module: every module in Pytorch subclasses the nn.Module. A neural network itself is a module that consists of other modules (layers).\n",
        "\n",
        "In the following, we will show how to build a neural network in Pytorch step by step.\n"
      ],
      "metadata": {
        "id": "C7m-7C9A6568"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iXAjkaRG6l1y"
      },
      "outputs": [],
      "source": [
        "# Import any libraries needed\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Topic 1: Get device for training**\n",
        "\n",
        "We would like to train our model on a hardware accelerator like the GPU or mps. For the device module, it will first check whether GPU or mps is available, if not, then choose CPU.\n",
        "\n",
        "GPU is checked through ***torch.cuda***, and mps is checked through ***torch.backends.mps***."
      ],
      "metadata": {
        "id": "EGIJw10FGaR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drNKY9naHmg_",
        "outputId": "8710a65b-d195-40ea-8196-272127de3dd1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Topic2: Define the Class**\n",
        "\n",
        "The neural network is defined by subclassing nn.Module. The neural network class should at least contains two methods as:\n",
        "\n",
        "\n",
        "> \\_\\_init\\_\\_: initializes all neural network layers.\n",
        "\n",
        "\n",
        "> forward: defines operations on input data.\n"
      ],
      "metadata": {
        "id": "SNZOilD0IXfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.linear_relu_stack = nn.Sequential(\n",
        "        nn.Linear(28*28, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 10),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.flatten(x)\n",
        "    logits = self.linear_relu_stack(x)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "efHKl1QpJyDs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creat an instance of NeuralNetwork, and move it to the device\n",
        "model = NeuralNetwork().to(device)\n",
        "# Print model's structure\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSisjPVBLF_0",
        "outputId": "00bc483c-7785-44ca-8ba9-011cbfafba73"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use the model, we pass it the input data. This executes the model's forward, along with some background operations.\n",
        "\n",
        "DO NOT CALL model.forward() DIRECTLY!"
      ],
      "metadata": {
        "id": "-8mSfxouQMMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(1, 28, 28, device=device)\n",
        "logits = model(x) # logits is a two-dimension tensor with dim=0 corresponding to each output of 10 raw predicted values for each class,\n",
        "          # dim=1 corresponding to the individual values of each output\n",
        "pred_prob = nn.Softmax(dim=1)(logits)\n",
        "y_pred = pred_prob.argmax(1)\n",
        "print(f\"Predicted class: {y_pred}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rr-l6MGUQtUg",
        "outputId": "671fc621-1504-483f-dd6c-2df82f2f565c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: tensor([0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Topic 3: Model Layers**\n",
        "\n",
        "Pytorch supports several layers with a simple API. Here we start with a sample minibatch of 3 images of size 28Ã—28 to see how some specific layers work"
      ],
      "metadata": {
        "id": "OdEihqnzbaTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the input data\n",
        "input_image = torch.rand(3, 28, 28)\n",
        "print(input_image.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2Wh2dN9iA9j",
        "outputId": "65f99c32-d259-4c17-ab72-acd64525f191"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***nn.Flatten()***: convert data of multiple dimensions into a contiguous array with only 1 dimension, just like flat the data. (the minibatch dimension, as dim=0, is maintained)"
      ],
      "metadata": {
        "id": "YGx6izrojO7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flatten = nn.Flatten()\n",
        "flat_image = flatten(input_image)\n",
        "print(flat_image.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8aNWjSAjwH2",
        "outputId": "d8217987-48d3-4156-91fd-2404f81a38fe"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 784])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***nn.Linear()***: applied linear transformations on the input using its stored weights and biases."
      ],
      "metadata": {
        "id": "4bTCoCdb13rK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
        "hidden1 = layer1(flat_image)\n",
        "print(hidden1.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-E70M81y2vzS",
        "outputId": "35b314b7-011c-49d4-eab7-be327610210d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***nn.ReLU()***: activation functions are applied after linear transformations to introduce non-linearity into the neural netowrk. It creates the complex mappings between inputs and outputs, enabling the neural network to learn a variety of phenomena.\n",
        "\n",
        "relu is just one of the activation functions, some other activation functions contain sigmoid, tanh, relu, leaky relu, elu, prelu, softplus, and so on\n"
      ],
      "metadata": {
        "id": "uH9_nkT63A3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Before relu: {hidden1}\")\n",
        "relu1 = nn.ReLU()(hidden1)\n",
        "print(f\"After relu: {relu1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E45LhIY66Uak",
        "outputId": "f2f3862b-2956-426f-947b-e8eac6ad8196"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before relu: tensor([[ 0.5179,  0.3581,  0.2407,  0.1851, -0.0105, -0.5799, -0.1621,  0.2630,\n",
            "          0.5479, -0.4820,  0.0849, -0.4071,  0.3334, -0.4439,  0.0151, -0.0371,\n",
            "          0.5095, -0.4157, -0.2217, -0.4470],\n",
            "        [ 0.1123,  0.5256,  0.5164, -0.0685, -0.4289, -0.7043, -0.0721,  0.3884,\n",
            "          0.3303, -0.5106,  0.5004, -0.0073,  0.2316, -0.0795,  0.0021, -0.0821,\n",
            "          0.4403, -0.2475, -0.2706, -0.1518],\n",
            "        [ 0.2741,  0.4296,  0.5632, -0.0423, -0.3454, -0.5177, -0.0436,  0.4500,\n",
            "          0.2363, -0.2600,  0.3312, -0.0431,  0.1478,  0.2601,  0.0545,  0.1024,\n",
            "          0.1976, -0.3729, -0.3928, -0.4924]], grad_fn=<AddmmBackward0>)\n",
            "After relu: tensor([[0.5179, 0.3581, 0.2407, 0.1851, 0.0000, 0.0000, 0.0000, 0.2630, 0.5479,\n",
            "         0.0000, 0.0849, 0.0000, 0.3334, 0.0000, 0.0151, 0.0000, 0.5095, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.1123, 0.5256, 0.5164, 0.0000, 0.0000, 0.0000, 0.0000, 0.3884, 0.3303,\n",
            "         0.0000, 0.5004, 0.0000, 0.2316, 0.0000, 0.0021, 0.0000, 0.4403, 0.0000,\n",
            "         0.0000, 0.0000],\n",
            "        [0.2741, 0.4296, 0.5632, 0.0000, 0.0000, 0.0000, 0.0000, 0.4500, 0.2363,\n",
            "         0.0000, 0.3312, 0.0000, 0.1478, 0.2601, 0.0545, 0.1024, 0.1976, 0.0000,\n",
            "         0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***nn.Sequential()***: an ordered container of modules. Data is passed through all modules following the order defined by nn.Sequential."
      ],
      "metadata": {
        "id": "QcU-qauK-Axe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq_module = nn.Sequential(\n",
        "    flatten,\n",
        "    layer1,\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(20, 10)\n",
        ")\n",
        "input_image = torch.rand(3, 28, 28)\n",
        "output = seq_module(input_image)\n",
        "print(output.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-AR6Qqi-clh",
        "outputId": "05768583-4700-48fe-da5e-e3c8fd277014"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***nn.Softmax()***: scaled logits into value [0, 1] to represent the model's predicted probabilities for each class. dim parameters indicates the dimension along which the sum of values is 1"
      ],
      "metadata": {
        "id": "_2mC7GW6APuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "softmax = nn.Softmax(dim=1)\n",
        "logits = softmax(output)\n",
        "print(logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2i-JfiiAmbV",
        "outputId": "20a1492b-9dc0-4a99-b5ce-d8cad132cc41"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0918, 0.1181, 0.0705, 0.0777, 0.1167, 0.1393, 0.0629, 0.1111, 0.1191,\n",
            "         0.0928],\n",
            "        [0.0967, 0.1220, 0.0736, 0.0781, 0.1152, 0.1424, 0.0631, 0.1037, 0.1146,\n",
            "         0.0905],\n",
            "        [0.0875, 0.1259, 0.0669, 0.0764, 0.1144, 0.1537, 0.0515, 0.1055, 0.1046,\n",
            "         0.1135]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Topic 4: Model Parameters**\n",
        "\n",
        "Many layers inside the neural netowrks are parameterized: i.e. have associated weights and biases that are optimized during training. nn.Module automatically tracked all fields inside the neural network, and make parameters accessible using ***parameters()*** or ***named_parameters()*** methods."
      ],
      "metadata": {
        "id": "vQzVJAX8BNEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Model structure: \\n{model}\\n\")\n",
        "\n",
        "for name, param in  model.named_parameters():\n",
        "  print(f\"Layer: {name}|Size: {param.size()}|Value: {param[:2]}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCmkiNtCB3Ww",
        "outputId": "80548861-7432-4137-b695-bc3c64717673"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model structure: \n",
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "Layer: linear_relu_stack.0.weight|Size: torch.Size([512, 784])|Value: tensor([[-0.0047, -0.0289, -0.0276,  ..., -0.0083, -0.0299, -0.0084],\n",
            "        [ 0.0179, -0.0047,  0.0353,  ...,  0.0047, -0.0096,  0.0104]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "\n",
            "Layer: linear_relu_stack.0.bias|Size: torch.Size([512])|Value: tensor([0.0064, 0.0269], grad_fn=<SliceBackward0>)\n",
            "\n",
            "Layer: linear_relu_stack.2.weight|Size: torch.Size([512, 512])|Value: tensor([[ 0.0111, -0.0299,  0.0100,  ...,  0.0328, -0.0416, -0.0039],\n",
            "        [-0.0281,  0.0113,  0.0401,  ...,  0.0386,  0.0225,  0.0413]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "\n",
            "Layer: linear_relu_stack.2.bias|Size: torch.Size([512])|Value: tensor([-0.0404, -0.0239], grad_fn=<SliceBackward0>)\n",
            "\n",
            "Layer: linear_relu_stack.4.weight|Size: torch.Size([10, 512])|Value: tensor([[ 0.0346, -0.0349, -0.0015,  ..., -0.0046, -0.0045,  0.0384],\n",
            "        [-0.0092,  0.0343,  0.0441,  ...,  0.0363,  0.0144, -0.0365]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "\n",
            "Layer: linear_relu_stack.4.bias|Size: torch.Size([10])|Value: tensor([0.0409, 0.0399], grad_fn=<SliceBackward0>)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}